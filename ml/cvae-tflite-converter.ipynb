{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ndo4ERqnwQOU"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfNT-mlFwxVM"
   },
   "source": [
    "# Convolutional Variational Autoencoder Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TD5ZrvEMbhZ"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/generative/cvae\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cvae.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cvae.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/generative/cvae.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "MTKwbguKwT4R"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITZuApL56Mny"
   },
   "source": [
    "![evolution of output during training](https://tensorflow.org/images/autoencoders/cvae.gif)\n",
    "\n",
    "This notebook demonstrates how to generate images of handwritten digits by training a Variational Autoencoder ([1](https://arxiv.org/abs/1312.6114), [2](https://arxiv.org/abs/1401.4082)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-JuIu2N_SQf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# to generate gifs\n",
    "!pip install -q imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "## Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "## Load the MNIST dataset\n",
    "Each MNIST image is originally a vector of 784 integers, each of which is between 0-255 and represents the intensity of a pixel. We model each pixel with a Bernoulli distribution in our model, and we statically binarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4fYMGxGhrna"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "#(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFC2ghIdiZYE"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalizing the images to the range of [0., 1.]\n",
    "train_images /= 255.\n",
    "test_images /= 255.\n",
    "\n",
    "# Binarization\n",
    "train_images[train_images >= .5] = 1.\n",
    "train_images[train_images < .5] = 0.\n",
    "test_images[test_images >= .5] = 1.\n",
    "test_images[test_images < .5] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4PIDhoDLbsZ"
   },
   "outputs": [],
   "source": [
    "TRAIN_BUF = 60000\n",
    "BATCH_SIZE = 32\n",
    "TEST_BUF = 10000\n",
    "\n",
    "epochs = 200\n",
    "latent_dim = 50\n",
    "conditional_vae = True\n",
    "\n",
    "dir_path = \"{:s}vae/z-{:03d}\".format(\"cond-\" if conditional_vae else \"\", latent_dim)\n",
    "\n",
    "if not os.path.exists(dir_path):\n",
    "    raise ValueError(\"Directory not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PIGN6ouoQxt3"
   },
   "source": [
    "## Use *tf.data* to create batches and shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yKCCQOoJ7cn"
   },
   "outputs": [],
   "source": [
    "if conditional_vae:\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)\n",
    "                                                      ).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)\n",
    "                                                     ).shuffle(TEST_BUF).batch(BATCH_SIZE)\n",
    "else:\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(TEST_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Wire up the generative and inference network with *tf.keras.Sequential*\n",
    "\n",
    "In our VAE example, we use two small ConvNets for the generative and inference network. Since these neural nets are small, we use `tf.keras.Sequential` to simplify our code. Let $x$ and $z$ denote the observation and latent variable respectively in the following descriptions.\n",
    "\n",
    "### Generative Network\n",
    "This defines the generative model which takes a latent encoding as input, and outputs the parameters for a conditional distribution of the observation, i.e. $p(x|z)$. Additionally, we use a unit Gaussian prior $p(z)$ for the latent variable.\n",
    "\n",
    "### Inference Network\n",
    "This defines an approximate posterior distribution $q(z|x)$, which takes as input an observation and outputs a set of parameters for the conditional distribution of the latent representation. In this example, we simply model this distribution as a diagonal Gaussian. In this case, the inference network outputs the mean and log-variance parameters of a factorized Gaussian (log-variance instead of the variance directly is for numerical stability).\n",
    "\n",
    "### Reparameterization Trick\n",
    "During optimization, we can sample from $q(z|x)$ by first sampling from a unit Gaussian, and then multiplying by the standard deviation and adding the mean. This ensures the gradients could pass through the sample to the inference network parameters.\n",
    "\n",
    "### Network architecture\n",
    "For the inference network, we use two convolutional layers followed by a fully-connected layer. In the generative network, we mirror this architecture by using a fully-connected layer followed by three convolution transpose layers (a.k.a. deconvolutional layers in some contexts). Note, it's common practice to avoid using batch normalization when training VAEs, since the additional stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGLbvBEmjK0a"
   },
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, conditional_vae=False):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.conditional_vae = conditional_vae\n",
    "        self.inference_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(28, 28, 1+10 if self.conditional_vae else 1)),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=32,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"SAME\",\n",
    "                    activation=tf.nn.leaky_relu),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=64,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"SAME\",\n",
    "                    activation=tf.nn.leaky_relu),\n",
    "                #tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                # No activation\n",
    "                tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.generative_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(\n",
    "                    input_shape=(latent_dim+10 if self.conditional_vae else latent_dim,)),\n",
    "                tf.keras.layers.Dense(units=7*7*64, activation=tf.nn.relu),\n",
    "                #tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Reshape(target_shape=(7, 7, 64)),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"SAME\",\n",
    "                    activation='relu'),\n",
    "                #tf.keras.layers.BatchNormalization(),\n",
    "                # No activation\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"SAME\"),\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def sample(self, eps=None, y=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, y=y, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x, y=None):\n",
    "        if self.conditional_vae:\n",
    "            # turn the classes numbers into one-hot vectors then reshape the vectors for further operations\n",
    "            y = tf.reshape(tf.one_hot(y, 10), (y.shape[0], 1, 1, 10))\n",
    "            # inputs shape will be (batch_size, 28, 28, 10+1)\n",
    "            inputs = tf.concat([x, y * tf.ones([y.shape[0],28,28,10])], 3)\n",
    "        else:\n",
    "            inputs = x        \n",
    "        mean, logvar = tf.split(self.inference_net(inputs), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        # logvar = log(sigma^2)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, y=None, apply_sigmoid=False):\n",
    "        if self.conditional_vae:\n",
    "            # turn the classes numbers into one-hot vectors\n",
    "            y = tf.one_hot(y, 10)\n",
    "            # inputs_k shape will be (batch_size, k, latent_dim+10)\n",
    "            inputs = tf.concat([z, y], 1)\n",
    "        else:\n",
    "            inputs = z\n",
    "        logits = self.generative_net(inputs)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the loss function and the optimizer\n",
    "\n",
    "VAEs train by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood:\n",
    "\n",
    "$$\\log p(x) \\ge \\text{ELBO} = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x, z)}{q(z|x)}\\right].$$\n",
    "\n",
    "In practice, we optimize the single sample Monte Carlo estimate of this expectation:\n",
    "\n",
    "$$\\log p(x| z) + \\log p(z) - \\log q(z|x),$$\n",
    "where $z$ is sampled from $q(z|x)$.\n",
    "\n",
    "**Note**: we could also analytically compute the KL term, but here we incorporate all three terms in the Monte Carlo estimator for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=-1):\n",
    "    #      normal_pdf = 1/(sigma*(2*pi)^1/2) * exp(-0.5*(x-mu)^2/sigma^2)\n",
    "    # log(normal_pdf) = -log(sigma) - 0.5*log(2*pi) -0.5*(x-mu)^2/sigma^2\n",
    "    #                 = -0.5 * ((x-mu)^2/sigma^2 + 2*log(sigma) + log(2*pi))\n",
    "    #                 = -0.5 * ((x-mu)^2/exp(logvar) + logvar + log(2*pi)), where logvar = log(sigma^2)\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis)\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(model, x, y=None):\n",
    "    mean, logvar = model.encode(x, y=y)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_logits = model.decode(z, y=y)\n",
    "\n",
    "    # NOTE: The shapes of log(p(x|z)), log(p(z)) & log(g(z|x)) are (batch size, k)\n",
    "    #\n",
    "    # With logistic regression (i.e., binarized data), x_hat = sigmoid(logits), x = labels:\n",
    "    #     cross entropy = x * -log(x_hat) + (1 - x) * -log(1 - x_hat)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logits, labels=x)\n",
    "    #      p(x|z) = Bernoulli(x;DNN(z)) = Bernoulli(x;x_hat)\n",
    "    #             = sum(x_hat^x * (1-x_hat)^(1-x))\n",
    "    # log(p(x|z)) = sum(x*log(x_hat) + (1-x)*log(1-x_hat))\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "        \n",
    "    # NOTE: the shape of log_weight is (batch size, k) \n",
    "    log_weights = logpx_z + logpz - logqz_x\n",
    "    \n",
    "    ### VAE\n",
    "    ### loss = -E[ average( log( p(zi)*p(x|zi)/q(zi|x) ) ) ] = -E[ average( log(p(zi)) + log(p(x|zi)) - log(q(zi|x)) ) ]\n",
    "    return -tf.reduce_mean(log_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Generate Images\n",
    "\n",
    "* After training, it is time to generate some images\n",
    "* We start by sampling a set of latent vectors from the unit Gaussian prior distribution $p(z)$\n",
    "* The generator will then convert the latent sample $z$ to logits of the observation, giving a distribution $p(x|z)$\n",
    "* Here we plot the probabilities of Bernoulli distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M7LmLtGEMQJ",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 32)        3200      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               313700    \n",
      "=================================================================\n",
      "Total params: 335,396\n",
      "Trainable params: 335,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 3136)              191296    \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 210,049\n",
      "Trainable params: 210,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CVAE(latent_dim, conditional_vae)\n",
    "model.inference_net.summary()\n",
    "model.generative_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning phase to False\n",
    "tf.keras.backend.set_learning_phase(False)\n",
    "    \n",
    "model.load_weights(os.path.join(dir_path, \"epcho-{:03d}.ckpt\".format(epochs)))\n",
    "\n",
    "if False:\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for test_x in test_dataset:\n",
    "        if isinstance(test_x, tuple):\n",
    "            loss(compute_loss(model, test_x[0], test_x[1]))\n",
    "        else:\n",
    "            loss(compute_loss(model, test_x))\n",
    "    elbo = -loss.result()\n",
    "    print(\"Test set ELBO: {}\".format(elbo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the keras model to TensorFlow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Keras model to TF Lite format.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model.inference_net)\n",
    "tflite_encode = converter.convert()\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model.generative_net)\n",
    "tflite_decode = converter.convert()\n",
    "\n",
    "with open('encode.tflite', \"wb\") as f:\n",
    "    f.write(tflite_encoder)\n",
    "with open('decode.tflite', \"wb\") as f:\n",
    "    f.write(tflite_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def _reparameterize(packed_data):\n",
    "    mean, logvar = tf.split(\n",
    "        tf.reshape(packed_data, packed_data.shape[1:]), num_or_size_splits=2, axis=0)\n",
    "    return tf.random.normal(shape=mean.shape) * tf.exp(logvar * .5) + mean\n",
    "\n",
    "@tf.function\n",
    "def _bufferize(logits):\n",
    "    norm_img = tf.sigmoid(tf.reshape(logits, logits.shape[1:-1]))\n",
    "    img = tf.cast(norm_img * 255, tf.uint8)\n",
    "    return img\n",
    "\n",
    "@tf.function\n",
    "def _one_hot_encode(main_param, label):\n",
    "    if main_param.shape[-1] != 1 and main_param.shape[-1] != latent_dim:\n",
    "        main_param = tf.reshape(main_param, (1,)+main_param.shape+(1,))\n",
    "    else:\n",
    "        main_param = tf.reshape(main_param, (1,)+main_param.shape)\n",
    "    onehot_label = tf.one_hot(label, 10)\n",
    "    expanded_onehot = tf.reshape(onehot_label,(1,)*(len(main_param.shape)-1)+(10,))\n",
    "    return tf.concat([main_param, tf.tile(expanded_onehot, main_param.shape[:-1]+(1,))],\n",
    "                     len(main_param.shape)-1)\n",
    "\n",
    "# Convert contrete functions\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions(\n",
    "    [_reparameterize.get_concrete_function(tf.TensorSpec(shape=[1,latent_dim*2], dtype=tf.float32))])\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "tflite_reparameterize = converter.convert()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions(\n",
    "    [_bufferize.get_concrete_function(tf.TensorSpec(shape=[1,28,28,1], dtype=tf.float32))])\n",
    "tflite_bufferize = converter.convert()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions(\n",
    "    [_one_hot_encode.get_concrete_function(main_param=tf.TensorSpec(shape=[28,28], dtype=tf.float32),\n",
    "                                      label=tf.TensorSpec(shape=[1], dtype=tf.int32))])\n",
    "tflite_enc_onehotencode = converter.convert()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions(\n",
    "    [_one_hot_encode.get_concrete_function(main_param=tf.TensorSpec(shape=[latent_dim], dtype=tf.float32),\n",
    "                                      label=tf.TensorSpec(shape=[1], dtype=tf.int32))])\n",
    "tflite_dec_onehotencode = converter.convert()\n",
    "\n",
    "with open('reparameterize.tflite', \"wb\") as f:\n",
    "    f.write(tflite_reparameterize)\n",
    "with open('bufferize.tflite', \"wb\") as f:\n",
    "    f.write(tflite_bufferize)\n",
    "with open('enc_onehotencode.tflite', \"wb\") as f:\n",
    "    f.write(tflite_enc_onehotencode)\n",
    "with open('dec_onehotencode.tflite', \"wb\") as f:\n",
    "    f.write(tflite_dec_onehotencode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run encode/decode with TensorFlow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAA6dJREFUeJzt3ctKJEEURVFT/P9fDkcFDorykUY+Yq817EG3CpsLfQzdxhhvwPrez/4AgGOIHSLEDhFihwixQ8THkf/Ytm3+6x8mG2Nsz/7cZYcIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEHPqjpOGr2b9UdNue/kTlLJcdIsQOEWKHCLFDhNghQuwQIXaIsLMz1cwt3Y7+Oy47RIgdIsQOEWKHCLFDhNghQuwQYWdnFzv6fbjsECF2iBA7RIgdIsQOEWKHCLFDhJ2dl+zo63DZIULsECF2iBA7RIgdIsQOEaa3OL82ucNlhwixQ4TYIULsECF2iBA7RIgdIuzsi7Oj8+CyQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4R3rMvwK9V5idcdogQO0SIHSLEDhFihwixQ4TYIcLOfgN2dP6Dyw4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iPDE9QJmPmGFB5cdIsQOEWKHCLFDhNghQuwQIXaIsLMv7so/KvrM7y+48tdlFpcdIsQOEWKHCLFDhNghQuwQIXaIsLMvYM9mXH1L/93nveIO77JDhNghQuwQIXaIEDtEiB0ixA4RdvYDnLlln72j+x6A63DZIULsECF2iBA7RIgdIsQOEaY3dpn5FPS7v9s09zsuO0SIHSLEDhFihwixQ4TYIULsEGFnX9ydfyTyzB39zl+Xv3LZIULsECF2iBA7RIgdIsQOEWKHCDv7AbzLfq76eZ/FZYcIsUOE2CFC7BAhdogQO0SIHSLs7At4tVfPfrd95lZefJO+h8sOEWKHCLFDhNghQuwQIXaIML1dwMwnsJ6R8uCyQ4TYIULsECF2iBA7RIgdIsQOEXb2G9jzlHP2zu6Z6X247BAhdogQO0SIHSLEDhFihwixQ4SdfXF2cB5cdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDxDbGOPtjAA7gskOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4Rn5a8VBGs/+f+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# turn the classes numbers into one-hot vectors then reshape the vectors for further operations\n",
    "index = 6000\n",
    "plt.imshow(train_images[index][:,:,0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# one hot encode\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_enc_onehotencode)\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], tf.reshape(train_images[index], (28,28)))\n",
    "interpreter.set_tensor(interpreter.get_input_details()[1][\"index\"], tf.cast([train_labels[index]], tf.int32))\n",
    "interpreter.invoke()\n",
    "input_data = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])()\n",
    "\n",
    "# image/label encode\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_encode)\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], input_data)\n",
    "interpreter.invoke()\n",
    "packed_code = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])()\n",
    "\n",
    "# reparameterize\n",
    "# NOTE: Python doesn't support tf lite with tf ops, so we cannot use the reparameterize concrete function here.\n",
    "#       For android, see https://www.tensorflow.org/lite/guide/ops_select#building_the_android_aar for more info.\n",
    "#interpreter = tf.lite.Interpreter(model_content=tflite_reparameterize)\n",
    "#interpreter.allocate_tensors()\n",
    "#interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], packed_code)\n",
    "#interpreter.invoke()\n",
    "#latent_code = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])()[0]\n",
    "latent_code = _reparameterize(packed_code)\n",
    "latent_code = tf.constant([0.17280698,0.1923644,-0.7641352,0.55188537,2.2787306,-1.3262384,-1.0158308,-1.8910618,-0.39106315,0.5028641,0.0831455,0.81001985,0.55455744,-0.78946334,-1.2557342,-0.91259134,-0.12709807,0.5213583,-1.3465408,1.9998562,-0.06604886,-0.5885987,3.3006577,-0.74014515,-0.04764238,-0.51263994,0.7322791,-0.5304394,0.56204045,-0.48698014,0.37121075,0.4936036,1.4419427,-0.27472275,0.5689496,-2.0289996,-0.31137896,-1.6851118,0.172723,-0.21331896,-0.7173588,1.5938954,-0.03326562,0.682987,1.5201209,-1.2610844,0.7547746,1.1410023,1.7021033,-0.040731817])\n",
    "\n",
    "# one hot encode\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_dec_onehotencode)\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], latent_code)\n",
    "interpreter.set_tensor(interpreter.get_input_details()[1][\"index\"], tf.cast([train_labels[index]], tf.int32))\n",
    "interpreter.invoke()\n",
    "input_data = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])()\n",
    "\n",
    "# image/label decode\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_decode)\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], input_data)\n",
    "interpreter.invoke()\n",
    "output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])()\n",
    "\n",
    "# bufferize\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_bufferize)\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], output)\n",
    "interpreter.invoke()\n",
    "output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])()\n",
    "\n",
    "plt.imshow(output, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARUAAAD4CAYAAADCQ3IKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABIJJREFUeJzt3T1O41AUgNF4NBUVLXthAUjsgoWxCiRWwF4oqWg9NZoIO5NvHrFzTkksnB/06UpcvUzzPB8AKr9++gkA+yIqQEpUgJSoAClRAVK/R95smib/aoKdmOd5OvZzkwqQEhUgJSpASlSAlKgAKVEBUqICpEQFSIkKkBIVICUqQEpUgJSoAClRAVKiAqREBUgNPaQJSmu+Xmaajp4jxH9kUgFSogKkRAVIiQqQEhUgJSpASlSAlKgAKctv/Ig1i2tsk0kFSIkKkBIVICUqQEpUgJSoAClRAVKiAqQsv5EbtdjmVLfLZFIBUqICpEQFSIkKkBIVICUqQEpUgJSoACnLb5zEYhtLTCpASlSAlKgAKVEBUqICpEQFSIkKkLKnwhf2UDiXSQVIiQqQEhUgJSpASlSAlKgAKVEBUqICpCy/XRGLbYxgUgFSogKkRAVIiQqQEhUgJSpASlSAlKgAKctvnMRiG0tMKkBKVICUqAApUQFSogKkRAVIiQqQsqeyE9UBTPZQOJdJBUiJCpASFSAlKkBKVICUqAApUQFSogKkLL9xkR4fHxeveXl5WbzGMt94JhUgJSpASlSAlKgAKVEBUqICpEQFSIkKkLL8tgFbO9Xt9fV18ZqHh4cBz6R57yzQncakAqREBUiJCpASFSAlKkBKVICUqAApUQFSlt84SbWIt+T9/X3xmru7u8Vriud7c3OzeM3n5+fZ99kLkwqQEhUgJSpASlSAlKgAKVEBUqICpKZReweHw+EwTdO4m+3IyEOaRv09XNLBRw5y+jfzPB990SYVICUqQEpUgJSoAClRAVKiAqREBUiJCpBySNMVucbFtjU+Pj6+ffz29nbxd6x5b7f2vvwrkwqQEhUgJSpASlSAlKgAKVEBUqICpEQFSDn5bQMGf0bD7rUVI0/e2xInvwFDiAqQEhUgJSpASlSAlKgAKVEBUqICpJz8dkX2tnzFZTKpAClRAVKiAqREBUiJCpASFSAlKkDKnsoV8S16jGBSAVKiAqREBUiJCpASFSAlKkBKVICUqAApy28bsGYhrfoWvefn528ff3p6Su7DfplUgJSoAClRAVKiAqREBUiJCpASFSAlKkBqqpamVt1smsbdjL+M+qy3dnpc8b5s7TUX5nk++qJNKkBKVICUqAApUQFSogKkRAVIiQqQckjTFRl12NOovY+RO1asZ1IBUqICpEQFSIkKkBIVICUqQEpUgJSoACnLb3yxtHQ2auHskhbbrvEApnOYVICUqAApUQFSogKkRAVIiQqQEhUgJSpAyvIbJxm1CLZm+e3+/n7xmre3t+LpcAKTCpASFSAlKkBKVICUqAApUQFSogKkRAVIWX7jIjltbbtMKkBKVICUqAApUQFSogKkRAVIiQqQEhUgJSpASlSAlKgAKVEBUqICpEQFSIkKkBIVIDWt+SY4gLVMKkBKVICUqAApUQFSogKkRAVIiQqQEhUgJSpASlSAlKgAKVEBUqICpEQFSIkKkBIVICUqQEpUgJSoAClRAVKiAqREBUiJCpD6A4bqigUjSXSlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = tf.reshape(tf.constant(\n",
    "    [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.29411766,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.7176471,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,1.0,1.0,0.90588236,0.0,1.0,1.0,0.039215688,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.9372549,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.61960787,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.23137255,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n",
    "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "), (25,28))\n",
    "#tmp = tf.clip_by_value(tmp, clip_value_min=0, clip_value_max=1)\n",
    "\n",
    "plt.imshow(tmp, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cvae.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
